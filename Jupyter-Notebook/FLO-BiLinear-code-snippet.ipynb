{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d86361f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import copy\n",
    "\n",
    "from typing import Dict, List, Tuple, Optional, Set\n",
    "import torch\n",
    "from torch import nn  \n",
    "from torch.utils import data  \n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455c87e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66567f16",
   "metadata": {
    "code_folding": [
     0,
     34,
     44
    ]
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim=1, hidden_dim=[512,512], act_func=nn.ReLU()):\n",
    "        super(MLP,self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.act_func = act_func\n",
    "        \n",
    "        layers = []\n",
    "        for i in range(len(hidden_dim)):\n",
    "            if i==0:\n",
    "                layer = nn.Linear(input_dim, hidden_dim[i])\n",
    "            else:\n",
    "                layer = nn.Linear(hidden_dim[i-1], hidden_dim[i])\n",
    "            nn.init.xavier_uniform_(layer.weight)\n",
    "            nn.init.zeros_(layer.bias)\n",
    "            layers.append(layer)\n",
    "            #layers.append(nn.ReLU(True))\n",
    "            layers.append(act_func)\n",
    "        if len(hidden_dim):                #if there is more than one hidden layer\n",
    "            layer = nn.Linear(hidden_dim[-1], output_dim)\n",
    "        else:\n",
    "            layer = nn.Linear(input_dim, output_dim)\n",
    "        nn.init.xavier_uniform_(layer.weight)\n",
    "        nn.init.zeros_(layer.bias)\n",
    "        layers.append(layer)\n",
    "        \n",
    "        self._main = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = x.view(x.shape[0], self.input_dim)\n",
    "        out = self._main(out)\n",
    "        return out\n",
    "\n",
    "class TwoInputMLPWrapper(nn.Module):\n",
    "    def __init__(self, func):\n",
    "        super(TwoInputMLPWrapper,self).__init__()\n",
    "        self.func = func\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        xy = torch.cat([x,y],dim=1)\n",
    "        \n",
    "        return self.func(xy)\n",
    "    \n",
    "def l2_normalizer(x, dim=1):\n",
    "    \n",
    "    norm = torch.sqrt(torch.square(x).sum(dim=dim,keepdim=True))\n",
    "    x_norm = x / norm\n",
    "    \n",
    "    return x_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601770f9",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class BilinearCritic(nn.Module):\n",
    "    \n",
    "    '''\n",
    "    encoder_x : dx -> feature_dim\n",
    "    encoder_y : dy -> feature_dim\n",
    "    u_func : 2*feature_dim -> 1\n",
    "    '''\n",
    "    \n",
    "    def __init__(self,\n",
    "                 encoder_x: nn.Module,\n",
    "                 encoder_y: nn.Module,\n",
    "                 u_func: nn.Module,\n",
    "                 tau: Optional[float] = 1.):\n",
    "        \n",
    "        super(BilinearCritic,self).__init__()\n",
    "        self.encoder_x = encoder_x\n",
    "        self.encoder_y = encoder_y\n",
    "        self.u_func = u_func\n",
    "        self.tau = torch.nn.Parameter(torch.Tensor([tau]))\n",
    "        \n",
    "    \n",
    "    def forward(self, x, y, tau=None):\n",
    "        if tau is None:\n",
    "            tau = self.tau\n",
    "        tau = torch.sqrt(tau)\n",
    "        hx = self.norm(self.encoder_x(x))\n",
    "        hy = self.norm(self.encoder_y(y))\n",
    "        u = self.u_func(hx,hy)\n",
    "        \n",
    "        return hx/tau, hy/tau, u  \n",
    "    \n",
    "    def norm(self,z):\n",
    "        return torch.nn.functional.normalize(z,dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c03fab",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class BilinearFLO(nn.Module):\n",
    "    \n",
    "    def __init__(self,\n",
    "         critic: nn.Module, \n",
    "         u_func: Optional[nn.Module] = None,\n",
    "         K: Optional[int] = None,\n",
    "         args: Optional[Dict] = None,\n",
    "         cuda: Optional[int] = None) -> None:\n",
    "        \n",
    "        super(BilinearFLO,self).__init__()\n",
    "        self.critic = critic\n",
    "        self.u_func = u_func\n",
    "        self.K = K\n",
    "    \n",
    "    def forward(self, x, y, y0,K=None):\n",
    "        \n",
    "        '''\n",
    "        x:    n x p\n",
    "        y:    n x d true\n",
    "        y0:   n x d fake \n",
    "        returns negative MI (i.e., can be directly used as loss for minimization)\n",
    "        '''\n",
    "        if K is None:\n",
    "            K = self.K \n",
    "        output  = self.PMI(x,y,y0,K)\n",
    "        return output.mean()\n",
    "    \n",
    "    def MI(self, x, y, K=10):\n",
    "        mi = 0\n",
    "        for k in range(K):\n",
    "            y0 = y[torch.randperm(y.size()[0])]\n",
    "            mi += self.forward(x,y,y0)\n",
    "            \n",
    "        return -mi/K    \n",
    "    \n",
    "    def PMI(self, x, y, y0=None, K=None):\n",
    "        '''\n",
    "        x:    n x p\n",
    "        y:    n x d true\n",
    "        y0:   n x d fake \n",
    "        '''\n",
    "\n",
    "        if self.u_func is not None:\n",
    "            u  = self.u_func(x, y)\n",
    "            if K is not None:\n",
    "            \n",
    "                for k in range(K-1):\n",
    "\n",
    "                    if k==0:\n",
    "                        y0 = y0\n",
    "                        g0 = self.critic(x, y0)\n",
    "                    else:\n",
    "                        y0 = y[torch.randperm(y.size()[0])]\n",
    "                        g0 = torch.cat((g0,self.critic(x, y0)),1)\n",
    "\n",
    "                g0_logsumexp = torch.logsumexp(g0,1).view(-1,1)\n",
    "                output = u + torch.exp(-u+g0_logsumexp-g)/(K-1) - 1\n",
    "            else:               \n",
    "                \n",
    "                g = self.critic(x, y)\n",
    "                g0 = self.critic(x, y0)\n",
    "               \n",
    "                output = u + torch.exp(-u+g0-g) - 1\n",
    "        else:\n",
    "            # one func mode\n",
    "            gu = self.critic(x,y)\n",
    "            if isinstance(gu, tuple):\n",
    "                hx,hy,u = gu\n",
    "                similarity_matrix = hx @ hy.t()\n",
    "                pos_mask = torch.eye(hx.size(0),dtype=torch.bool)\n",
    "                g = similarity_matrix[pos_mask].view(hx.size(0),-1)\n",
    "                g0 = similarity_matrix[~pos_mask].view(hx.size(0),-1)\n",
    "                g0_logsumexp = torch.logsumexp(g0,1).view(-1,1)\n",
    "                output = u + torch.exp(-u+g0_logsumexp-g)/(hx.size(0)-1) - 1\n",
    "\n",
    "            else:      \n",
    "                g, u = torch.chunk(self.critic(x,y),2,dim=1)\n",
    "                if K is not None:\n",
    "\n",
    "                    for k in range(K-1):\n",
    "\n",
    "                        if k==0:\n",
    "                            y0 = y0\n",
    "                            g0,_ = torch.chunk(self.critic(x,y0),2,dim=1)\n",
    "                        else:\n",
    "                            y0 = y[torch.randperm(y.size()[0])]\n",
    "                            g00,_ = torch.chunk(self.critic(x,y0),2,dim=1)\n",
    "                            g0 = torch.cat((g0,g00),1)\n",
    "\n",
    "                    g0_logsumexp = torch.logsumexp(g0,1).view(-1,1)\n",
    "                    output = u + torch.exp(-u+g0_logsumexp-g)/(K-1) - 1\n",
    "                else:    \n",
    "\n",
    "                    g0, _ = torch.chunk(self.critic(x,y0),2,dim=1)\n",
    "                    output = u + torch.exp(-u+g0-g) - 1\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e01dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {}\n",
    "args['lr'] = 1e-3\n",
    "args['latent_dim'] = 100\n",
    "args['num_epochs'] = int(50*np.sqrt(k/16))\n",
    "args[\"input_dim\"] = 2*p\n",
    "args['output_dim'] = 2\n",
    "args['batch_size'] = k\n",
    "args['feature_dim'] = 512\n",
    "\n",
    "encoder_x = MLP(p, output_dim=args['feature_dim'])\n",
    "encoder_y = MLP(p, output_dim=args['feature_dim'])\n",
    "u_func = TwoInputMLPWrapper(MLP(2*args['feature_dim'],hidden_dim=[128]))\n",
    "critic = BilinearCritic(encoder_x, encoder_y, u_func)\n",
    "model = BilinearFLO(critic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d0b955",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "912b3bc8",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def LossFLO(feat1, feat2, u_func, inv_temp=1., feat2_transposed=False, normalizer=None):\n",
    "    \n",
    "    '''\n",
    "    feat1   bs1 x dim\n",
    "    feat2   bs2 x dim (dim x bs2 if feat2_tranposed is True)\n",
    "    normalizer(x, dim=1)\n",
    "    \n",
    "    Official implementation of\n",
    "    Qing Guo, et al. Tight Mutual Information Estimation With Contrastive Fenchel-Legendre Optimization\n",
    "    NeurIPS 2022\n",
    "    https://arxiv.org/abs/2107.01131\n",
    "    \n",
    "    bs1<=bs2, and (input1[i], input2[i]) are positive pairs\n",
    "    all (input1[i], input2[j]) i!=j are negative pairs\n",
    "    when bs1<bs2, the negative samples are augmented (e.g., momentum contrastive (MoCo))\n",
    "    '''\n",
    "    \n",
    "    assert len(feat1.size())==2, 'input1 dimension should be batch_size x feature_dim'\n",
    "    assert len(feat2.size())==2, 'input2 dimension should be batch_size x feature_dim (or transpose)'\n",
    "    \n",
    "    if feat2_transposed is False:\n",
    "        feat2 = feat2.t()\n",
    "        \n",
    "    assert feat1.size(dim=1)==feat2.size(dim=0), 'The feature dimension should match for input1 and input2'\n",
    "    \n",
    "    n1 = feat1.size(dim=0)\n",
    "    n2 = feat2.size(dim=1)\n",
    "    assert n1<=n2, 'Size of input2 should not be less than input1'\n",
    "    \n",
    "    # Normlize feature if normalizer is specified\n",
    "    if normalizer is not None:\n",
    "        feat1 = normalizer(feat1, dim=1)\n",
    "        feat2 = normalizer(feat2, dim=0)\n",
    "    \n",
    "    similarity = feat1 @ feat2\n",
    "    \n",
    "    mask = torch.eye(n1, dtype=torch.bool)\n",
    "    if n1<n2:\n",
    "        mask = torch.cat([mask,torch.zeros([n1,n2-n1], dtype=torch.bool)], dim=1)\n",
    "    \n",
    "    positives = similarity[mask].view(n1,-1)\n",
    "    negatives = similarity[~mask].view(n1,-1)\n",
    "    \n",
    "    g = positives * inv_temp\n",
    "    g0 = negatives * inv_temp\n",
    "    \n",
    "    u = u_func(feat1, feat2[:,:n1].t())\n",
    "    \n",
    "    g0_logsumexp = torch.logsumexp(g0, dim=1, keepdim=True)\n",
    "    \n",
    "    loss_vec = u + torch.exp(-u+g0_logsumexp-g)/(n2-1) - 1\n",
    "    loss = loss_vec.mean()\n",
    "    \n",
    "    res = dict()\n",
    "    res['loss_vec'] = loss_vec\n",
    "    res['similarity'] = similarity\n",
    "    res['u'] = u\n",
    "    \n",
    "    return loss, res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fcbd43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4da27aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs1 = 10\n",
    "bs2 = 15\n",
    "dim = 20\n",
    "\n",
    "u_func = TwoInputMLPWrapper(MLP(2*dim,hidden_dim=[128]))\n",
    "\n",
    "x1 = torch.Tensor(np.random.randn(bs1,dim))\n",
    "x2 = torch.Tensor(np.random.randn(bs2,dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dd90a7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_flo, res = LossFLO(x1, x2, u_func, normalizer=l2_normalizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cytao",
   "language": "python",
   "name": "cytao"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
